{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4swWFX6GSOx",
        "outputId": "b54b3cc9-5c87-4188-9a1c-11a94c789c90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 423ms/step - loss: 0.0241 - val_loss: 0.0092\n",
            "Epoch 2/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 408ms/step - loss: 0.0087 - val_loss: 0.0079\n",
            "Epoch 3/10\n",
            "\u001b[1m167/391\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:27\u001b[0m 390ms/step - loss: 0.0077"
          ]
        }
      ],
      "source": [
        "### Task 1: Image Quality Enhancement & Restoration\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import cifar100\n",
        "\n",
        "# Load CIFAR-100 dataset\n",
        "(x_train, _), (x_test, _) = cifar100.load_data()\n",
        "\n",
        "# Normalize images\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Introduce noise\n",
        "noise_factor = 0.2\n",
        "x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)\n",
        "x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)\n",
        "x_train_noisy = np.clip(x_train_noisy, 0.0, 1.0)\n",
        "x_test_noisy = np.clip(x_test_noisy, 0.0, 1.0)\n",
        "\n",
        "# Define Autoencoder model\n",
        "def build_autoencoder():\n",
        "    input_img = Input(shape=(32, 32, 3))\n",
        "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
        "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
        "    encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)\n",
        "    x = UpSampling2D((2, 2))(x)\n",
        "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = UpSampling2D((2, 2))(x)\n",
        "    decoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "    return Model(input_img, decoded)\n",
        "\n",
        "# Compile and train autoencoder\n",
        "autoencoder = build_autoencoder()\n",
        "autoencoder.compile(optimizer=Adam(), loss='mse')\n",
        "autoencoder.fit(x_train_noisy, x_train, epochs=10, batch_size=128, validation_data=(x_test_noisy, x_test))\n",
        "\n",
        "# Evaluate\n",
        "predicted = autoencoder.predict(x_test_noisy)\n",
        "psnr_value = tf.image.psnr(tf.convert_to_tensor(x_test), tf.convert_to_tensor(predicted), max_val=1.0)\n",
        "print(\"Average PSNR:\", np.mean(psnr_value.numpy()))\n",
        "\n",
        "# Display Results\n",
        "n = 5\n",
        "plt.figure(figsize=(10, 5))\n",
        "for i in range(n):\n",
        "    ax = plt.subplot(3, n, i + 1)\n",
        "    plt.imshow(x_test_noisy[i])\n",
        "    ax.set_title(\"Noisy\")\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "    ax = plt.subplot(3, n, i + 1 + n)\n",
        "    plt.imshow(predicted[i])\n",
        "    ax.set_title(\"Restored\")\n",
        "    ax.axis(\"off\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "### Task 2: Image Classification using AlexNet and VGG16\n",
        "\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "\n",
        "# Load CIFAR-100 dataset\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
        "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
        "\n",
        "def train_model(model, criterion, optimizer, num_epochs=5):\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for images, labels in trainloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(trainloader)}\")\n",
        "\n",
        "def evaluate_model(model):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in testloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    print(f\"Accuracy: {100 * correct / total}%\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# AlexNet\n",
        "alexnet = models.alexnet(pretrained=True)\n",
        "alexnet.classifier[6] = nn.Linear(4096, 100)\n",
        "alexnet = alexnet.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(alexnet.parameters(), lr=0.001)\n",
        "train_model(alexnet, criterion, optimizer)\n",
        "evaluate_model(alexnet)\n",
        "\n",
        "# VGG16\n",
        "vgg16 = models.vgg16(pretrained=True)\n",
        "vgg16.classifier[6] = nn.Linear(4096, 100)\n",
        "vgg16 = vgg16.to(device)\n",
        "optimizer = optim.Adam(vgg16.parameters(), lr=0.001)\n",
        "train_model(vgg16, criterion, optimizer)\n",
        "evaluate_model(vgg16)"
      ],
      "metadata": {
        "id": "sEPcA2D6GX-I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}